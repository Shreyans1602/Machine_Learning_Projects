{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information\n",
    "Authors: Shreyans Patel (SSP210009) and Pranitha Sreethar (PXS200095)\n",
    "\n",
    "Dataset Owner/Donor Information:\n",
    "\n",
    "Name: Amir Karami\n",
    "\n",
    "Institutions: University of South Carolina\n",
    "\n",
    "Email: karami@sc.edu \n",
    "\n",
    "Date Donated: 2015 \n",
    "\n",
    "Dataset Information:\n",
    "\n",
    "The data was collected in 2015 using Twitter API. This dataset contains health news from more than 15 major health news agencies such as BBC, CNN, and NYT.\n",
    "\n",
    "References:\n",
    "\n",
    "1) https://numpy.org/doc/stable/reference/\n",
    "\n",
    "2) https://pandas.pydata.org/docs/reference/index.html\n",
    "\n",
    "3) https://docs.python.org/3/library/re.html\n",
    "\n",
    "4) https://docs.python.org/3/library/random.html\n",
    "\n",
    "5) https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index from which the actual tweet starts. This will enable us to remove tweet id and timestamp without using regex\n",
    "TWEET_START_INDEX = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Total tweets:  3929 \n",
      "\n",
      "Pre-processing the data\n",
      "\n",
      "['breast', 'cancer', 'risk', 'test', 'devised']\n",
      "['gp', 'workload', 'harming', 'care', 'bma', 'poll']\n",
      "['short', 'people', 's', 'heart', 'risk', 'greater']\n",
      "['new', 'approach', 'against', 'hiv', 'promising']\n",
      "['coalition', 'undermined', 'nhs', 'doctors']\n",
      "['review', 'of', 'case', 'against', 'nhs', 'manager']\n",
      "['video', 'all', 'day', 'is', 'empty', 'what', 'am', 'i', 'going', 'to', 'do']\n",
      "['video', 'overhaul', 'needed', 'for', 'end', 'of', 'life', 'care']\n",
      "['care', 'for', 'dying', 'needs', 'overhaul']\n",
      "['video', 'nhs', 'labour', 'and', 'tory', 'key', 'policies']\n",
      "['have', 'gp', 'services', 'got', 'worse']\n",
      "['a&amp;e', 'waiting', 'hits', 'new', 'worst', 'level']\n",
      "['parties', 'row', 'over', 'gp', 'opening', 'hours']\n",
      "['why', 'strenuous', 'runs', 'may', 'not', 'be', 'so', 'bad', 'after', 'all']\n",
      "['video', 'health', 'surcharge', 'for', 'non', 'eu', 'patients']\n",
      "['video', 'skin', 'cancer', 'spike', 'from', '60s', 'holidays']\n",
      "['80', '000', 'might', 'die', 'in', 'future', 'outbreak']\n",
      "['skin', 'cancer', 'linked', 'to', 'holiday', 'boom']\n",
      "['public', 'back', 'tax', 'rises', 'to', 'fund', 'nhs']\n",
      "['video', 'welcome', 'to', 'the', 'designer', 'asylum']\n",
      "['video', 'why', 'are', 'we', 'having', 'less', 'sex']\n",
      "['five', 'ideas', 'to', 'transform', 'the', 'nhs']\n",
      "['personal', 'cancer', 'vaccines', 'exciting']\n",
      "['child', 'heart', 'surgery', 'deaths', 'halved']\n",
      "['video', 'miliband', 'cameron', 'failed', 'the', 'nhs']\n",
      "['unsafe', 'food', 'growing', 'global', 'threat']\n",
      "['health', 'highlights']\n",
      "['ambulance', 'progress', 'not', 'fast', 'enough']\n",
      "['children', 's', 'hospital', 'builds', 'sleep', 'app']\n",
      "['drug', 'giant', 'blocks', 'eye', 'treatment']\n",
      "\n",
      "Pre-processed data successfully\n",
      "\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3643.4338520874544\n",
      "SSE Error for iteration 2 :3688.0060841278396\n",
      "SSE Error for iteration 3 :3465.5787602564183\n",
      "SSE Error for iteration 4 :3380.5112190945365\n",
      "Final SSE Error:  3380.5112190945365\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3691.7550954189064\n",
      "SSE Error for iteration 2 :3581.4070176419577\n",
      "SSE Error for iteration 3 :3413.3526503501153\n",
      "SSE Error for iteration 4 :3227.3920037132534\n",
      "Final SSE Error:  3227.3920037132534\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3608.7142930889822\n",
      "SSE Error for iteration 2 :3558.025576159655\n",
      "SSE Error for iteration 3 :3637.4106677827385\n",
      "SSE Error for iteration 4 :3525.8408137027427\n",
      "SSE Error for iteration 5 :3128.262314367031\n",
      "Final SSE Error:  3128.262314367031\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3569.755544834364\n",
      "SSE Error for iteration 2 :3537.5007190849883\n",
      "SSE Error for iteration 3 :3158.718613805683\n",
      "SSE Error for iteration 4 :3074.0080414513764\n",
      "Final SSE Error:  3074.0080414513764\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3716.4009941491277\n",
      "SSE Error for iteration 2 :3624.8980173741934\n",
      "SSE Error for iteration 3 :3065.546100143766\n",
      "Final SSE Error:  3065.546100143766\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3861.720600454674\n",
      "SSE Error for iteration 2 :3765.7549173490434\n",
      "SSE Error for iteration 3 :3566.109150080482\n",
      "SSE Error for iteration 4 :3070.996719163603\n",
      "SSE Error for iteration 5 :3044.7919090362307\n",
      "Final SSE Error:  3044.7919090362307\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3780.342104104913\n",
      "SSE Error for iteration 2 :3773.5596139882905\n",
      "SSE Error for iteration 3 :3254.6366866611306\n",
      "SSE Error for iteration 4 :2979.5968858357705\n",
      "Final SSE Error:  2979.5968858357705\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3823.5458055398203\n",
      "SSE Error for iteration 2 :3786.5066984039518\n",
      "SSE Error for iteration 3 :3393.3635504154895\n",
      "SSE Error for iteration 4 :2972.6307757752847\n",
      "Final SSE Error:  2972.6307757752847\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3867.0234530835633\n",
      "SSE Error for iteration 2 :3678.167338073522\n",
      "SSE Error for iteration 3 :3259.0264842996794\n",
      "SSE Error for iteration 4 :3165.8808265655557\n",
      "SSE Error for iteration 5 :2933.099136083353\n",
      "Final SSE Error:  2933.099136083353\n",
      "\n",
      "Training stage started\n",
      "\n",
      "SSE Error for iteration 1 :3808.944246149341\n",
      "SSE Error for iteration 2 :3420.0851332347097\n",
      "SSE Error for iteration 3 :3363.0400584023937\n",
      "SSE Error for iteration 4 :2902.2382844878853\n",
      "Final SSE Error:  2902.2382844878853\n",
      "\n",
      "Printing the required output table:\n",
      "\n",
      "    Value_of_K    SSE_Error                                   Size_of_Clusters\n",
      "1            5  3380.511219          {0: 1914, 1: 895, 2: 264, 3: 237, 4: 619}\n",
      "2           10  3227.392004  {0: 1434, 1: 692, 2: 519, 3: 124, 4: 120, 5: 2...\n",
      "3           15  3128.262314  {0: 1257, 1: 221, 2: 317, 3: 619, 4: 90, 5: 11...\n",
      "4           20  3074.008041  {0: 802, 1: 144, 2: 220, 3: 164, 4: 22, 5: 547...\n",
      "5           25  3065.546100  {0: 988, 1: 26, 2: 255, 3: 116, 4: 246, 5: 125...\n",
      "6           30  3044.791909  {0: 563, 1: 102, 2: 155, 3: 262, 4: 307, 5: 45...\n",
      "7           35  2979.596886  {0: 616, 1: 89, 2: 103, 3: 499, 4: 197, 5: 320...\n",
      "8           40  2972.630776  {0: 795, 1: 52, 2: 51, 3: 86, 4: 213, 5: 31, 6...\n",
      "9           45  2933.099136  {0: 791, 1: 48, 2: 65, 3: 76, 4: 299, 5: 342, ...\n",
      "10          50  2902.238284  {0: 439, 1: 84, 2: 32, 3: 96, 4: 184, 5: 17, 6...\n"
     ]
    }
   ],
   "source": [
    "class K_Means_Clustering:\n",
    "    def __init__(self, data_file):\n",
    "        # Get data and split into seperate tweets\n",
    "        self.raw_input = requests.get(data_file)\n",
    "        self.raw_input = self.raw_input.text\n",
    "        self.raw_input = self.raw_input.split('\\n')\n",
    "        \n",
    "        # Loaded Successfully\n",
    "        print(\"Data loaded successfully. Total tweets: \", len(self.raw_input), \"\\n\")\n",
    "\n",
    "    # Helper Functions Start\n",
    "    def get_jaccard_dist(self, tweet_1, tweet_2):\n",
    "        # Take intersection\n",
    "        intersection = len(list(set(tweet_1).intersection(tweet_2)))\n",
    "\n",
    "        # Take union\n",
    "        union = (len(tweet_1) + len(tweet_2)) - intersection\n",
    "        return float(1 - (intersection / union))\n",
    "\n",
    "    def clusterize(self, list_of_list_of_tweets, centroids): \n",
    "        # Dictionary of clusters\n",
    "        dict_of_words = {}\n",
    "\n",
    "        # For tweet_1 and tweet_2, compute their jaccard distance and then add the closest tweets to disctionary\n",
    "        for tweet_1 in list_of_list_of_tweets:\n",
    "            jaccard_dist = []\n",
    "            for tweet_2 in range(len(centroids)):\n",
    "                jaccard_dist.append(self.get_jaccard_dist(centroids[tweet_2], tweet_1))\n",
    "            min_dist = jaccard_dist.index(min(jaccard_dist))\n",
    "            dict_of_words.setdefault(min_dist, [])\n",
    "            dict_of_words[min_dist].append(tweet_1)\n",
    "        return dict_of_words \n",
    "\n",
    "    def calculate_centroid(self, list_of_list_of_tweets):\n",
    "        min_dists = {}\n",
    "        idx = 0\n",
    "        for tweet_1 in list_of_list_of_tweets:\n",
    "            list_of_min_dist = []\n",
    "            for tweet_2 in list_of_list_of_tweets:\n",
    "                dist = self.get_jaccard_dist(tweet_1, tweet_2)\n",
    "                list_of_min_dist.append(dist) \n",
    "            min_dists[idx] = sum(list_of_min_dist)\n",
    "            idx = idx + 1\n",
    "        min_dist_cluster_idx = [(key, value)[0] for key, value in min_dists.items() if value == min(min_dists.values())]\n",
    "        return list_of_list_of_tweets[min_dist_cluster_idx[0]]\n",
    "\n",
    "    def calculate_sse_error(self, centroids, clusters):\n",
    "        sse_error = 0\n",
    "        # print(\"Clusters:\", clusters.keys())\n",
    "        # print(\"Centroids:\", centroid)\n",
    "        for key, value in clusters.items():\n",
    "            for data_point in value:\n",
    "                sse_error += self.get_jaccard_dist(centroids[key], data_point)**2\n",
    "\n",
    "        return sse_error\n",
    "    # Helper Functions End\n",
    "\n",
    "    def pre_process(self):\n",
    "        print(\"Pre-processing the data\\n\")\n",
    "        self.processed_data = []\n",
    "        \n",
    "        for tweet in self.raw_input:\n",
    "            # Remove the tweet id and timestamp\n",
    "            modified_tweet = tweet[TWEET_START_INDEX:]\n",
    "\n",
    "            # Remove any word that starts with @\n",
    "            modified_tweet = regex.sub('@\\S+', \"\", modified_tweet)\n",
    "\n",
    "            # Remove any hastag symbols from words\n",
    "            modified_tweet = regex.sub('#', \"\", modified_tweet)\n",
    "\n",
    "            # Remove any url \n",
    "            modified_tweet = regex.sub('http\\S+', '', modified_tweet)\n",
    "\n",
    "            # Convert every word to lowercase\n",
    "            modified_tweet = modified_tweet.lower()\n",
    "\n",
    "            # Also remove any other symbols since we believe it is introducing noise\n",
    "            modified_tweet = regex.sub(r'[\\'’‘\\\"?,:-]', ' ', modified_tweet)\n",
    "\n",
    "            # Split the tweet into a list of words\n",
    "            modified_tweet = modified_tweet.split()\n",
    "\n",
    "            self.processed_data.append(modified_tweet)\n",
    "        \n",
    "        # Check if pre_process is successful by printing first 30 tweets\n",
    "        for string in self.processed_data[:30]:\n",
    "            print(string)\n",
    "\n",
    "        print(\"\\nPre-processed data successfully\\n\")\n",
    "\n",
    "    def train(self, k):\n",
    "        print(\"\\nTraining stage started\\n\")\n",
    "        centroids = []\n",
    "        centroids_new = []\n",
    "        random_idx = random.sample(range(0, len(self.processed_data) - 1), k)\n",
    "        # print(\"Random: \",random_idx) # Check if random is working fine\n",
    "        for idx in random_idx:\n",
    "            centroids.append(self.processed_data[idx])\n",
    "        centroids_old = centroids\n",
    "        iteration = 1\n",
    "\n",
    "        while True:\n",
    "            centroids_new = []\n",
    "            clusters = self.clusterize(self.processed_data, centroids_old)\n",
    "            for words in clusters:\n",
    "                centroids_new.append(self.calculate_centroid(clusters[words]))\n",
    "            print(\"SSE Error for iteration\", iteration, \":\" + str(self.calculate_sse_error(centroids_new,clusters)))\n",
    "            iteration += 1\n",
    "            if centroids_old == centroids_new:\n",
    "                break\n",
    "            centroids_old = centroids_new\n",
    "            \n",
    "        return centroids_new, clusters\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # List of hyperparameters and results\n",
    "    values_of_k_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "    size_of_cluster_list = []\n",
    "    sse_error_list = []\n",
    "\n",
    "    # Get the data\n",
    "    KMC = K_Means_Clustering(\"https://raw.githubusercontent.com/Shreyans1602/Machine_Learning_K_Means_Clustering/main/BBC_Health_Dataset.txt\")\n",
    "    \n",
    "    # Pre-process the data\n",
    "    KMC.pre_process()\n",
    "\n",
    "    # Train\n",
    "    for k in values_of_k_list:\n",
    "        centroid, clusters = KMC.train(k)\n",
    "        size_of_clusters = {}\n",
    "        for cluster in clusters:\n",
    "            size_of_clusters[cluster] = len(clusters[cluster])\n",
    "        size_of_cluster_list.append(size_of_clusters)\n",
    "        sse_error_list.append(KMC.calculate_sse_error(centroid, clusters))\n",
    "        print(\"Final SSE Error: \", KMC.calculate_sse_error(centroid, clusters))\n",
    "\n",
    "    # Make a table to print results and export a csv\n",
    "    results_table = pd.DataFrame()\n",
    "    results_table[\"Value_of_K\"] = values_of_k_list\n",
    "    results_table[\"SSE_Error\"] = sse_error_list\n",
    "    results_table[\"Size_of_Clusters\"] = size_of_cluster_list\n",
    "    results_table.index = results_table.index + 1\n",
    "    results_table.to_csv('results.csv')\n",
    "    print(\"\\nPrinting the required output table:\\n\")\n",
    "    print(results_table)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
