"""
part2.ipynb

Automatically generated by Colaboratory.

Original file is located at https://colab.research.google.com/drive/1Y7UoHD_lSTrIXDF3iy3j09a7YhH6lcwd

# Information
Authors: Shreyans Patel (SSP210009) and Pranitha Sreethar (PXS200095)

Dataset Owner/Donor Information:
Name: Prof. I-Cheng Yeh 
Institutions: Department of Civil Engineering, Tamkang University, Taiwan 
Email: 140910@mail.tku.edu.tw 
TEL: 886-2-26215656 ext. 3181 
Date Donated: Aug. 18, 2018 

Dataset Information:
The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan. The real estate valuation is a regression problem. The data set was randomly split into the training data set (2/3 samples) and the testing data set (1/3 samples).

Target Variable:
Y = House price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)

References:
1) https://scikit-learn.org/stable/modules/classes.html
2) https://numpy.org/doc/stable/reference/
3) https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set
4) https://www.kaggle.com/dskagglemt/real-estate-valuation-using-linearsvr
5) https://medium.com/@powusu381/multiple-regression-in-python-using-scikit-learn-predicting-the-miles-per-gallon-mpg-of-cars-4c8e512234be
"""

# Commented out IPython magic to ensure Python compatibility.
# Library Imports
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as matplt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import explained_variance_score

# %matplotlib inline

# Dataset Loading
dataframe = pd.DataFrame(pd.read_excel("https://github.com/Shreyans1602/Machine_Learning_Linear_Regression/raw/main/Dataset.xlsx", sheet_name = 'Dataset', index_col = 'No'))

# Loaded Successfully
print("Data Loaded Successfully")
print("Real Estate Valuation Data Set has {} data points with {} variables each.".format(*dataframe.shape))

# Pre-Processing Stage
print("Pre-Processing the Data:\n")

# Check for null values in the dataframe
print("Null entries found?:", ("No\n" if dataframe.isnull().sum().sum() == 0 else "Yes\n"))

# Check for duplicate values in the dataframe
print("Duplicate entries found?:", ("No\n" if dataframe.duplicated().sum() == 0 else "Yes\n"))

# Check if there is any categorical values
print("Check for categorical values:")
print(dataframe.dtypes)

# Rename attributes and describe the dataframe
dataframe.rename(
    columns = {
        "X1 transaction date": "Transaction_Date", 
        "X2 house age": "House_Age", 
        "X3 distance to the nearest MRT station": "MRT_Distance",
        "X4 number of convenience stores": "Num_Stores_NearBy",
        "X5 latitude": "Latitude",
        "X6 longitude": "Longitude",
        "Y house price of unit area": "House_Price",
    },
    inplace = True
)

print("\nRenaming the attributes for convenience. The dataframe is as follows:\n")
print(dataframe.head())

print("\nDescription of the dataframe is as follows:")
print(dataframe.describe())

# Printing correlation matrix
print("\nCorrelation matrix is as follows:")
print(dataframe.corr())

# Show the impact of different attributes on the House_Price variable
print("\nMost impactful attributes on House_Price variable are shows below in decending order:")
print(abs(dataframe.corr())['House_Price'].sort_values(ascending = False))

# Show various plots for visualization of the above information. Un-Comment lines 97 to 108 to see the plots.
# sns.set(rc = {'figure.figsize':(18,10)})
# hmap = sns.heatmap(dataframe.corr(), vmin = -1, vmax = 1)

# # Checking the correlation of all the attributes vs the House_Price variable
# sns.barplot(y = dataframe.corr().loc['House_Price'].index, x = dataframe.corr().loc['House_Price'].values)

# # Show plots for effect of each variable on House_Price
# columns = dataframe.columns

# for i in range(len(columns) - 1):
#     matplt.figure(i)
#     sns.scatterplot(x = columns[i], y = 'House_Price', data = dataframe)

# Based on the above heatmap, correlation scatter and bar graphs
# High Correlation Attributes w.r.t target are Distance, Num_Stores_NearBy, Latitude, Longitude, House_Age.
# Neligible Correlation Attributes w.r.t target is No and Transaction_Date.

# Dropping the insignificant attributes from the data set
dataframe = dataframe.drop(['Transaction_Date'], axis = 1)
print(dataframe.columns)

# Prepare X and Y matrix
X = np.array(dataframe.drop(['House_Price'], axis = 1))
Y = np.array(dataframe['House_Price'])

# Split train and test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 99)
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

# Scaling the data set
std_scaler = StandardScaler()
std_scaler.fit(X_train)
X_train_scaled = std_scaler.transform(X_train)
X_test_scaled = std_scaler.transform(X_test)

# Train model with different parameters and log the results
# Training Parameters
train_learning_rates = [0.1, 0.01, 0.001]
train_iterations = [100, 120, 140, 160, 180, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
config_count = 1

# Training Results Arrays
mse_arr = []
rmse_arr = []
r2_arr = []
ev_arr = []

# Open Log File
log_file = open("logs_library_implementation.txt","w")

for i in train_learning_rates:
    for j in train_iterations:
        # Skip iterations less than or equal to 200 for learning rate = 0.001 to avoid failure in convergence warning
        if i == 0.001:
            if j <= 200:
                continue
                
        # Learning Rate as i and the max_iterations as j
        lrsgd = SGDRegressor(eta0 = i, max_iter = j, random_state = 99)
        lrsgd.fit(X_train_scaled, np.array(Y_train))
        Y_pred = lrsgd.predict(X_test_scaled)
        
        # Evaluate Performance
        mse = mean_squared_error(Y_test, Y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(Y_test, Y_pred)
        ev = explained_variance_score(Y_test, Y_pred)
        
        # Store Results
        mse_arr.append(mse)
        rmse_arr.append(rmse)
        r2_arr.append(r2)
        ev_arr.append(ev)
        
        # Log Data
        log_file.write("Run: " + str(config_count) + " || MSE: " + str(mse) + " || R^2 Score: " + str(r2) + " || Explained Variance Score: " + str(ev) + " || Learning Rate: " + str(i) + "|| Iterations: " + str(j) + "\n")
        config_count += 1

best_param_idx = mse_arr.index(min(mse_arr))
print("\nEvaluation Parameters:\n")
print("Best Performance Results:")
print("MSE:", mse_arr[best_param_idx])
print("RMSE:", rmse_arr[best_param_idx])
print("R^2 Score:", r2_arr[best_param_idx])
print("Explained Variance Score: ", ev_arr[best_param_idx])

print("\nBest Parameters for Model Training:")
print("Learning Rate:", train_learning_rates[(best_param_idx % len(train_learning_rates))])
print("Iterations:", train_iterations[(best_param_idx % len(train_iterations))])

log_file.close()

# Train model with same parameters as manual implementation

# Model Training Stage
print("\nTraining model with same parameters as manual implementation:")
lrsgd = SGDRegressor(eta0 = 0.01, max_iter = 100, random_state = 99)
lrsgd.fit(X_train_scaled, np.array(Y_train))
Y_pred = lrsgd.predict(X_test_scaled)

# Evaluate Performance
mse = mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(Y_test, Y_pred)
ev = explained_variance_score(Y_test, Y_pred)
        
print("\nEvaluation Parameters:\n")
print("Performance Results:")
print("MSE:", mse)
print("RMSE:", rmse)
print("R^2 Score:", r2)
print("Explained Variance Score: ", ev)

# Train model with default library parameters

# Model Training Stage
print("\nTraining model with default library parameters:")
lrsgd = SGDRegressor()
lrsgd.fit(X_train_scaled, np.array(Y_train))
Y_pred = lrsgd.predict(X_test_scaled)

# Evaluate Performance
mse = mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(Y_test, Y_pred)
ev = explained_variance_score(Y_test, Y_pred)
        
print("\nEvaluation Parameters:\n")
print("Best Performance Results:")
print("MSE:", mse)
print("RMSE:", rmse)
print("R^2 Score:", r2)
print("Explained Variance Score: ", ev)