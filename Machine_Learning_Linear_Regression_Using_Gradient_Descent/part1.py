"""
part1.ipynb

Automatically generated by Colaboratory.

Original file is located at https://colab.research.google.com/drive/1aOkGs5BEzxvngu6YS6azH5Lpd7lzMHpQ

# Information
Authors: Shreyans Patel (SSP210009) and Pranitha Sreethar (PXS200095)

Dataset Owner/Donor Information:
Name: Prof. I-Cheng Yeh 
Institutions: Department of Civil Engineering, Tamkang University, Taiwan 
Email: 140910@mail.tku.edu.tw 
TEL: 886-2-26215656 ext. 3181 
Date Donated: Aug. 18, 2018 

Dataset Information:
The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan. The real estate valuation is a regression problem. The data set was randomly split into the training data set (2/3 samples) and the testing data set (1/3 samples).

Target Variable:
Y = House price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)

References:
1) https://scikit-learn.org/stable/modules/classes.html
2) https://numpy.org/doc/stable/reference/
3) https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set
4) https://www.kaggle.com/dskagglemt/real-estate-valuation-using-linearsvr
5) https://medium.com/@powusu381/multiple-regression-in-python-using-scikit-learn-predicting-the-miles-per-gallon-mpg-of-cars-4c8e512234be
"""

# Commented out IPython magic to ensure Python compatibility.
# Library Imports
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as matplt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import explained_variance_score

# %matplotlib inline

class LinearRegression:
    # Initializes the learning rate and the max Iteration
    def __init__(self, learning_rate, max_iterations):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations

    # Function that finds the best fit line
    def bestfit(self, X, Y):
        self.X = X
        self.Y = Y

        # Create an array for the loss function
        self.cost = []

        # Adding a new columns for the x to hold X0 as 1
        self.weights = np.random.rand(self.X.shape[1] + 1).reshape(1, -1) 

        # Create the weights i.e. w0, w1, w2, w3 etc              
        self.new_X = np.insert(self.X.T, 0, np.ones(self.X.shape[0]), axis=0)  

        # Cost function  
        derivation_cost = 0
        
        while self.max_iterations > -1:
            # Finds the predicted value - woxo + w1x1 + w2x2 + ..... + wnxn
            self.hypothesis = np.dot(self.weights, self.new_X)

            # Append to the cost array
            self.cost.append(self.cost_function(self.hypothesis, Y))

            # Find the number of rows in X
            m = self.X.shape[0] 

            # Take derivative of the function
            derivation_cost = (self.new_X@(self.hypothesis - self.Y).T) * 1 / m

            # Theta = Theta - learning rate * derivative
            self.weights -= (self.learning_rate * derivation_cost.reshape(1, -1))
            self.max_iterations -= 1

    # Function to find the cost_value     
    def cost_function(self, X, Y):
        loss = np.sum(np.square(X.reshape(-1, 1) - Y.reshape(-1,1))) / (2 * X.shape[0])
        return np.round(loss, 3)

    # Function to find the r2_value   
    def r2_score(self,X,Y):
        return 1 - (((Y - self.predict(X)) ** 2).sum() / ((Y - Y.mean()) ** 2).sum())

    # Function to find the predicted value for X 
    def predict(self,X):
        X = np.insert(X.T, 0, np.ones(X.shape[0]), axis = 0)
        return np.dot(self.weights, X)

# Dataset Loading
dataframe = pd.DataFrame(pd.read_excel("https://github.com/Shreyans1602/Machine_Learning_Linear_Regression/raw/main/Dataset.xlsx", sheet_name = 'Dataset', index_col = 'No'))

# Loaded Successfully
print("Data Loaded Successfully")
print("Real Estate Valuation Data Set has {} data points with {} variables each.".format(*dataframe.shape))

# Pre-Processing Stage
print("Pre-Processing the Data:\n")

# Check for null values in the dataframe
print("Null entries found?:", ("No\n" if dataframe.isnull().sum().sum() == 0 else "Yes\n"))

# Check for duplicate values in the dataframe
print("Duplicate entries found?:", ("No\n" if dataframe.duplicated().sum() == 0 else "Yes\n"))

# Check if there is any categorical values
print("Check for categorical values:")
print(dataframe.dtypes)

# Rename attributes and describe the dataframe
dataframe.rename(
    columns = {
        "X1 transaction date": "Transaction_Date", 
        "X2 house age": "House_Age", 
        "X3 distance to the nearest MRT station": "MRT_Distance",
        "X4 number of convenience stores": "Num_Stores_NearBy",
        "X5 latitude": "Latitude",
        "X6 longitude": "Longitude",
        "Y house price of unit area": "House_Price",
    },
    inplace = True
)

# Print the description
print("\nRenaming the attributes for convenience. The dataframe is as follows:\n")
print(dataframe.head())

# Print dataset description
print("\nDescription of the dataframe is as follows:")
print(dataframe.describe())

# Printing correlation matrix
print("\nCorrelation matrix is as follows:")
print(dataframe.corr())

# Show the impact of different attributes on the House_Price variable
print("\nMost impactful attributes on House_Price variable are shows below in decending order:")
print(abs(dataframe.corr())['House_Price'].sort_values(ascending = False))

# Show various plots for visualization of the above information. Un-Comment lines 152 to 163 to see the plots.
# sns.set(rc = {'figure.figsize':(18,10)})
# hmap = sns.heatmap(dataframe.corr(), vmin = -1, vmax = 1)

# # Checking the correlation of all the attributes vs the House_Price variable
# sns.barplot(y = dataframe.corr().loc['House_Price'].index, x = dataframe.corr().loc['House_Price'].values)

# # Show plots for effect of each variable on House_Price
# columns = dataframe.columns

# for i in range(len(columns) - 1):
#     matplt.figure(i)
#     sns.scatterplot(x = columns[i], y = 'House_Price', data = dataframe)

# Based on the above heatmap, correlation scatter and bar graphs
# High Correlation Attributes w.r.t target are Distance, Num_Stores_NearBy, Latitude, Longitude, House_Age.
# Neligible Correlation Attributes w.r.t target is No and Transaction_Date.

# Dropping the insignificant attributes from the data set
dataframe = dataframe.drop(['Transaction_Date'], axis = 1)
print(dataframe.columns)

# Prepare X and Y matrix
X = np.array(dataframe.drop(['House_Price'], axis = 1))
Y = np.array(dataframe['House_Price'])

# Split train and test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 99)
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

# Scaling the data set
std_scaler = StandardScaler()
std_scaler.fit(X_train)
X_train_scaled = std_scaler.transform(X_train)
X_test_scaled = std_scaler.transform(X_test)

# Loss vs Iteration Plots for different Learning Rates. Un-Comment lines 188 to 222 to see the loss vs iterations plots.
# # 0.1
# linear_regressor = LinearRegression(learning_rate = 0.1, max_iterations = 1000)
# linear_regressor.bestfit(X_train_scaled, np.array(Y_train))
# Y_pred = linear_regressor.predict(X_test_scaled)

# loss = list(linear_regressor.cost)
# matplt.plot(loss)
# matplt.xlabel("Iterations")
# matplt.ylabel("Loss")
# matplt.title("Learning Rate: 0.1")
# matplt.show()

# # 0.01
# linear_regressor = LinearRegression(learning_rate = 0.01, max_iterations = 1000)
# linear_regressor.bestfit(X_train_scaled, np.array(Y_train))
# Y_pred = linear_regressor.predict(X_test_scaled)

# loss = list(linear_regressor.cost)
# matplt.plot(loss)
# matplt.xlabel("Iterations")
# matplt.ylabel("Loss")
# matplt.title("Learning Rate: 0.01")
# matplt.show()

# # 0.001
# linear_regressor = LinearRegression(learning_rate = 0.001, max_iterations = 1000)
# linear_regressor.bestfit(X_train_scaled, np.array(Y_train))
# Y_pred = linear_regressor.predict(X_test_scaled)

# loss = list(linear_regressor.cost)
# matplt.plot(loss)
# matplt.xlabel("Iterations")
# matplt.ylabel("Loss")
# matplt.title("Learning Rate: 0.001")
# matplt.show()

# Train model with different parameters and log the results
# Training Parameters
train_learning_rates = [0.001, 0.01, 0.1]
train_iterations = [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
config_count = 1

# Training Results Arrays
mse_arr = []
rmse_arr = []
r2_arr = []
ev_arr = []

# Open Log File
log_file = open("logs_manual_implementation.txt","w")

for i in train_learning_rates:
    for j in train_iterations:
        # Learning Rate as i and the max_iterations as j
        linear_regressor = LinearRegression(learning_rate = i, max_iterations = j)
        linear_regressor.bestfit(X_train_scaled, np.array(Y_train))
        Y_pred = linear_regressor.predict(X_test_scaled)
        
        # Evaluate Performance
        mse = mean_squared_error(Y_test, Y_pred[0])
        rmse = np.sqrt(mse)
        r2 = r2_score(Y_test, Y_pred[0])
        ev = explained_variance_score(Y_test, Y_pred[0])
        
        # Store Results
        mse_arr.append(mse)
        rmse_arr.append(rmse)
        r2_arr.append(r2)
        ev_arr.append(ev)
        
        # Log Data
        log_file.write("Run: " + str(config_count) + " || MSE: " + str(mse) + " || R^2 Score: " + str(r2) + " || Explained Variance Score: " + str(ev) + " || Learning Rate: " + str(i) + "|| Iterations: " + str(j) + "\n")
        config_count += 1

best_param_idx = mse_arr.index(min(mse_arr))
print("\nEvaluation Parameters:\n")
print("Best Performance Results:")
print("MSE:", mse_arr[best_param_idx])
print("RMSE:", rmse_arr[best_param_idx])
print("R^2 Score:", r2_arr[best_param_idx])
print("Explained Variance Score: ", ev_arr[best_param_idx])

print("\nBest Parameters for Model Training:")
print("Learning Rate:", train_learning_rates[(best_param_idx % len(train_learning_rates))])
print("Iterations:", train_iterations[(best_param_idx % len(train_iterations))])

log_file.close()